{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sajal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e7a784d1eba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreviews_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreviews_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreviews_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreviews_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/datasets/base.py\u001b[0m in \u001b[0;36mload_files\u001b[0;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_error\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "reviews_train = load_files('data/train/')\n",
    "reviews_test = load_files('data/test/')\n",
    "X_train,y_train = reviews_train.data,reviews_train.target\n",
    "X_test,y_test = reviews_test.data,reviews_test.target\n",
    "X = X_train + X_test\n",
    "y = np.concatenate([y_train,y_test])\n",
    "X = X[:50000]\n",
    "y = y[:50000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = open('X.pickle','rb')\n",
    "y_in = open('y.pickle','rb')\n",
    "X = pickle.load(X_in)\n",
    "y = pickle.load(y_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "uncheck_words = ['don','won','doesn','couldn','isn','wasn','wouldn','can','ain','shouldn','not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(X)):\n",
    "    antonyms = []\n",
    "    review = re.sub(r'\\W', ' ', str(X[i]))\n",
    "    review = re.sub(r'\\d', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = re.sub(r'br[\\s$]', ' ', review)\n",
    "    review = re.sub(r'\\s+[a-z][\\s$]', ' ',review)\n",
    "    review = re.sub(r'b\\s+', '', review)\n",
    "    review = re.sub(r'\\s+', ' ', review)\n",
    "    word_list = review.split(' ')\n",
    "    newword_list = []\n",
    "    temp_word = ''\n",
    "    for word in word_list:\n",
    "        if temp_word in uncheck_words:\n",
    "            if word not in stop_words:\n",
    "                word = 'not_' + word\n",
    "                temp_word = ''\n",
    "        if word in uncheck_words:\n",
    "            temp_word = word\n",
    "        if word not in uncheck_words:\n",
    "            newword_list.append(word)\n",
    "    review = ' '.join(newword_list)\n",
    "    corpus.append(review)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tiv = TfidfVectorizer(max_features = 2000, min_df = 2, norm=\"l2\", use_idf=True, sublinear_tf = True, max_df = 0.6, stop_words = stop_words)\n",
    "X = tiv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tiv = TfidfVectorizer(max_features = 8000,min_df = 2, norm=\"l2\", use_idf=True, sublinear_tf = True, max_df = 0.6, stop_words = stop_words)\n",
    "X = tiv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "text_train, text_test, sent_train, sent_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC(C = 0.1)\n",
    "classifier.fit(text_train,sent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svcclassifier.pickle','wb') as f:\n",
    "    pickle.dump(classifier,f)\n",
    "\n",
    "with open('TFIDF.pickle','wb') as f:\n",
    "    pickle.dump(tiv,f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pred = classifier.predict(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4467,  585],\n",
       "       [ 558, 4390]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(sent_test, sent_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8727818  0.88727818 0.88025    0.883      0.889      0.87625\n",
      " 0.88075    0.883      0.88922231 0.88572143]\n",
      "0.8827253720937733\n",
      "0.005121239218105936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = text_train, y = sent_train, cv = 10)\n",
    "print(accuracies)\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sample):\n",
    "    sample = nltk.sent_tokenize(sample[0])\n",
    "    corpus = []\n",
    "    for i in range(0, len(sample)):\n",
    "        review = re.sub(r'\\W', ' ', sample[i])\n",
    "        review = re.sub(r'\\d', ' ', review)\n",
    "        review = review.lower()\n",
    "        review = re.sub(r'br[\\s$]', ' ', review)\n",
    "        review = re.sub(r'\\s+[a-z][\\s$]', ' ',review)\n",
    "        review = re.sub(r'b\\s+', '', review)\n",
    "        review = re.sub(r'\\s+', ' ', review)\n",
    "        word_list = review.split(' ')\n",
    "        new_words = []\n",
    "        temp_word = ''\n",
    "        for word in word_list:\n",
    "            antonyms = []\n",
    "            if word == 'not':\n",
    "                temp_word = 'not_'\n",
    "            elif temp_word == 'not_':\n",
    "                for syn in wordnet.synsets(word):\n",
    "                    for s in syn.lemmas():\n",
    "                        for a in s.antonyms():\n",
    "                            antonyms.append(a.name())\n",
    "                if len(antonyms) >= 1:\n",
    "                    word = antonyms[0]\n",
    "                else:\n",
    "                    word = temp_word + word\n",
    "                temp_word = ''\n",
    "            if word != 'not':\n",
    "                new_words.append(word)\n",
    "        review = ' '.join(new_words)\n",
    "        corpus.append(review)\n",
    "        print(review)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the government has done great to make policy get implemented to grassroot level \n",
      "and are also forming new laws for brtter implementation of current policies \n",
      "so the intention of government looks good \n",
      "but there are still scope of improvernment when it comes to corruption \n",
      "the government should look into the matter with more interest \n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# Using our classifier\n",
    "with open('TFIDF.pickle','rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "    \n",
    "with open('svcclassifier.pickle','rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "    \n",
    "    \n",
    "# sample = ['''\n",
    "# The point is that government has made the policy, \n",
    "# but there is no one to make sure that it is reaching \n",
    "# those who actually need to be benifitted from this policy. \n",
    "# So, what  my small point is that government should farme certain policy to make these existing policies better. \n",
    "# Otherwise this country will remain in this state of mess the it has been for past 70 years. \n",
    "# There wouldn't be any difference in the way Britishers left it to way we are now.''']\n",
    "\n",
    "\n",
    "sample = ['''\n",
    "    the government has done great to make policy get implemented to grassroot level.\n",
    "    And are also forming new laws for brtter implementation of current\n",
    "    policies. So the intention of government looks good.\n",
    "    But there are still scope of improvernment, when it comes to corruption.\n",
    "    The government should look into the matter with more interest.\n",
    "''']\n",
    "\n",
    "\n",
    "sample = process_data(sample)\n",
    "sample = tfidf.transform(sample).toarray()\n",
    "sentiment = clf.predict(sample)\n",
    "print(sentiment.mean()*5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
